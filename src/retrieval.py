import json
import logging
from typing import List, Tuple, Dict, Union
from rank_bm25 import BM25Okapi
import pickle
from pathlib import Path
import faiss
from openai import OpenAI
from dotenv import load_dotenv
import os
import numpy as np
from src.reranking import LLMReranker

_log = logging.getLogger(__name__)

class BM25Retriever:
    def __init__(self, bm25_db_dir: Path, documents_dir: Path):
        # åˆå§‹åŒ–BM25æ£€ç´¢å™¨ï¼ŒæŒ‡å®šBM25ç´¢å¼•å’Œæ–‡æ¡£ç›®å½•
        self.bm25_db_dir = bm25_db_dir
        self.documents_dir = documents_dir
        
    def retrieve_by_company_name(self, company_name: str, query: str, top_n: int = 3, return_parent_pages: bool = False) -> List[Dict]:
        # æŒ‰å…¬å¸åæ£€ç´¢ç›¸å…³æ–‡æœ¬å—ï¼Œè¿”å›žBM25åˆ†æ•°æœ€é«˜çš„top_nä¸ªå—
        document_path = None
        for path in self.documents_dir.glob("*.json"):
            with open(path, 'r', encoding='utf-8') as f:
                doc = json.load(f)
                if doc["metainfo"]["company_name"] == company_name:
                    document_path = path
                    document = doc
                    break
                    
        if document_path is None:
            raise ValueError(f"No report found with '{company_name}' company name.")
            
        # åŠ è½½å¯¹åº”çš„BM25ç´¢å¼•
        bm25_path = self.bm25_db_dir / f"{document['metainfo']['sha1_name']}.pkl"
        with open(bm25_path, 'rb') as f:
            bm25_index = pickle.load(f)
            
        # èŽ·å–æ–‡æ¡£å†…å®¹å’ŒBM25ç´¢å¼•
        document = document
        chunks = document["content"]["chunks"]
        pages = document["content"]["pages"]
        
        # è®¡ç®—BM25åˆ†æ•°
        tokenized_query = query.split()
        scores = bm25_index.get_scores(tokenized_query)
        
        actual_top_n = min(top_n, len(scores))
        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:actual_top_n]
        
        retrieval_results = []
        seen_pages = set()
        
        for index in top_indices:
            score = round(float(scores[index]), 4)
            chunk = chunks[index]
            parent_page = next(page for page in pages if page["page"] == chunk["page"])
            
            if return_parent_pages:
                if parent_page["page"] not in seen_pages:
                    seen_pages.add(parent_page["page"])
                    result = {
                        "distance": score,
                        "page": parent_page["page"],
                        "text": parent_page["text"]
                    }
                    retrieval_results.append(result)
            else:
                result = {
                    "distance": score,
                    "page": chunk["page"],
                    "text": chunk["text"]
                }
                retrieval_results.append(result)
        
        return retrieval_results



class VectorRetriever:
    def __init__(self, vector_db_dir: Path, documents_dir: Path, embedding_provider: str = "dashscope"):
        # åˆå§‹åŒ–å‘é‡æ£€ç´¢å™¨ï¼ŒåŠ è½½æ‰€æœ‰å‘é‡åº“å’Œæ–‡æ¡£
        self.vector_db_dir = vector_db_dir
        self.documents_dir = documents_dir
        self.all_dbs = self._load_dbs()
        # é»˜è®¤ä½¿ç”¨ dashscope ä½œä¸º embedding provider
        self.embedding_provider = embedding_provider.lower()
        self.llm = self._set_up_llm()

    def _set_up_llm(self):
        # æ ¹æ® embedding_provider åˆå§‹åŒ–å¯¹åº”çš„ LLM å®¢æˆ·ç«¯
        # æ³¨æ„ï¼šä¸åœ¨è¿™é‡Œè®¾ç½®dashscope.api_keyï¼Œå› ä¸ºæ­¤æ—¶çŽ¯å¢ƒå˜é‡å¯èƒ½è¿˜æœªè®¾ç½®
        # è€Œæ˜¯åœ¨_get_embeddingä¸­æ¯æ¬¡ä½¿ç”¨æ—¶åŠ¨æ€è¯»å–
        load_dotenv()
        if self.embedding_provider == "openai":
            llm = OpenAI(
                api_key=os.getenv("OPENAI_API_KEY"),
                timeout=None,
                max_retries=2
            )
            return llm
        elif self.embedding_provider == "dashscope":
            # dashscopeä¸éœ€è¦clientå¯¹è±¡ï¼ŒAPIå¯†é’¥åœ¨_get_embeddingä¸­åŠ¨æ€è®¾ç½®
            return None
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ embedding provider: {self.embedding_provider}")

    def _get_embedding(self, text: str):
        # æ ¹æ® embedding_provider èŽ·å–æ–‡æœ¬çš„å‘é‡è¡¨ç¤º
        if self.embedding_provider == "openai":
            embedding = self.llm.embeddings.create(
                input=text,
                model="text-embedding-3-large"
            )
            return embedding.data[0].embedding
        elif self.embedding_provider == "dashscope":
            import dashscope
            # ç¡®ä¿APIå¯†é’¥å·²è®¾ç½® - æ¯æ¬¡ä½¿ç”¨æ—¶éƒ½é‡æ–°è¯»å–ï¼Œç¡®ä¿èŽ·å–æœ€æ–°å€¼
            api_key = os.getenv("DASHSCOPE_API_KEY")
            if not api_key:
                # å°è¯•ä»Ždashscopeæ¨¡å—èŽ·å–ï¼ˆå¦‚æžœä¹‹å‰è®¾ç½®è¿‡ï¼‰
                if hasattr(dashscope, 'api_key') and dashscope.api_key:
                    api_key = dashscope.api_key
                else:
                    raise RuntimeError("DASHSCOPE_API_KEYçŽ¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œè¯·åœ¨Streamlit Secretsä¸­é…ç½®")
            # åŽ»é™¤é¦–å°¾ç©ºæ ¼ï¼Œé˜²æ­¢æ ¼å¼é—®é¢˜
            api_key = str(api_key).strip()
            if not api_key:
                raise RuntimeError("DASHSCOPE_API_KEYä¸ºç©ºï¼Œè¯·æ£€æŸ¥Streamlit Secretsé…ç½®")
            
            # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥å¯†é’¥æ ¼å¼
            key_length = len(api_key)
            key_prefix = api_key[:10] if len(api_key) >= 10 else api_key
            key_suffix = api_key[-10:] if len(api_key) >= 10 else ""
            # æ£€æŸ¥æ˜¯å¦æœ‰ç‰¹æ®Šå­—ç¬¦ï¼ˆå¦‚æ¢è¡Œç¬¦ã€åˆ¶è¡¨ç¬¦ç­‰ï¼‰
            has_newline = '\n' in api_key or '\r' in api_key
            has_tab = '\t' in api_key
            
            # å¦‚æžœå¯†é’¥é•¿åº¦ä¸å¯¹æˆ–åŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼Œç»™å‡ºè­¦å‘Š
            if key_length != 64:
                _log.warning(f"APIå¯†é’¥é•¿åº¦å¼‚å¸¸: {key_length} (æœŸæœ›64), å‰ç¼€: {key_prefix}, åŽç¼€: {key_suffix}")
            if has_newline or has_tab:
                # æ¸…ç†ç‰¹æ®Šå­—ç¬¦
                api_key = api_key.replace('\n', '').replace('\r', '').replace('\t', '')
                api_key = api_key.strip()
                _log.warning(f"æ£€æµ‹åˆ°APIå¯†é’¥ä¸­åŒ…å«æ¢è¡Œç¬¦æˆ–åˆ¶è¡¨ç¬¦ï¼Œå·²æ¸…ç†ã€‚æ–°é•¿åº¦: {len(api_key)}")
            
            # æ¯æ¬¡è°ƒç”¨éƒ½é‡æ–°è®¾ç½®ï¼Œç¡®ä¿ä½¿ç”¨æœ€æ–°çš„å¯†é’¥
            dashscope.api_key = api_key
            rsp = dashscope.TextEmbedding.call(
                model="text-embedding-v1",
                input=[text]
            )
            # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºNone
            if rsp is None:
                raise RuntimeError("DashScope APIè¿”å›žNoneï¼Œå¯èƒ½æ˜¯APIå¯†é’¥æ— æ•ˆæˆ–ç½‘ç»œé—®é¢˜ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥é…ç½®")
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå­—å…¸æˆ–å¯¹è±¡ï¼Œå¹¶èŽ·å–çŠ¶æ€ç 
            status_code = None
            if isinstance(rsp, dict):
                status_code = rsp.get('status_code')
                code = rsp.get('code', '')
                message = rsp.get('message', '')
            elif hasattr(rsp, 'status_code'):
                status_code = rsp.status_code
                code = getattr(rsp, 'code', '')
                message = getattr(rsp, 'message', '')
            
            # å¦‚æžœçŠ¶æ€ç æ˜¯401ï¼Œè¯´æ˜ŽAPIå¯†é’¥æ— æ•ˆ
            if status_code == 401 or code == 'InvalidApiKey':
                # æ˜¾ç¤ºå¯†é’¥è°ƒè¯•ä¿¡æ¯ï¼ˆä¸æ˜¾ç¤ºå®Œæ•´å¯†é’¥ï¼‰
                debug_info = f"å¯†é’¥é•¿åº¦: {key_length}, å‰ç¼€: {key_prefix}, åŽç¼€: {key_suffix}"
                if has_newline or has_tab:
                    debug_info += f", æ£€æµ‹åˆ°ç‰¹æ®Šå­—ç¬¦å·²æ¸…ç†"
                
                raise RuntimeError(
                    f"âŒ DashScope APIå¯†é’¥æ— æ•ˆï¼\n"
                    f"é”™è¯¯ä»£ç : {code}\n"
                    f"é”™è¯¯ä¿¡æ¯: {message}\n"
                    f"è°ƒè¯•ä¿¡æ¯: {debug_info}\n\n"
                    f"è¯·æ£€æŸ¥ï¼š\n"
                    f"1. åœ¨Streamlit Cloudçš„Secretsä¸­é…ç½®äº†æ­£ç¡®çš„DASHSCOPE_API_KEY\n"
                    f"2. APIå¯†é’¥æ ¼å¼: DASHSCOPE_API_KEY = \"å®Œæ•´å¯†é’¥\"ï¼ˆä¸€è¡Œï¼Œç”¨å¼•å·åŒ…è£¹ï¼Œç­‰å·å‰åŽæœ‰ç©ºæ ¼ï¼‰\n"
                    f"3. ç¡®ä¿å¯†é’¥æ²¡æœ‰å¤šä½™ç©ºæ ¼æˆ–éšè—å­—ç¬¦\n"
                    f"4. APIå¯†é’¥æ²¡æœ‰è¿‡æœŸæˆ–è¢«ç¦ç”¨\n"
                    f"5. ä¿å­˜åŽç­‰å¾…1-2åˆ†é’Ÿè®©é…ç½®ç”Ÿæ•ˆ\n\n"
                    f"ðŸ’¡ æç¤ºï¼šå¦‚æžœæœ¬åœ°èƒ½è¿è¡Œä½†Streamlit Cloudä¸è¡Œï¼Œå¯èƒ½æ˜¯Secretsä¸­çš„å¯†é’¥æ ¼å¼æœ‰é—®é¢˜ã€‚"
                    f"è¯·åˆ é™¤Secretsä¸­çš„å†…å®¹ï¼Œé‡æ–°è¾“å…¥ï¼šDASHSCOPE_API_KEY = \"ä½ çš„å®Œæ•´å¯†é’¥\""
                )
            
            # å…¼å®¹ dashscope è¿”å›žæ ¼å¼ï¼Œå¯èƒ½è¿”å›žå¯¹è±¡æˆ–å­—å…¸
            # å…ˆå®‰å…¨èŽ·å–output
            output = None
            if hasattr(rsp, 'get'):
                output = rsp.get('output')
            elif hasattr(rsp, 'output'):
                output = rsp.output
            elif isinstance(rsp, dict) and 'output' in rsp:
                output = rsp['output']
            
            # æ£€æŸ¥embeddings
            if output and isinstance(output, dict) and 'embeddings' in output:
                # å¤šæ¡è¾“å…¥ï¼ˆæœ¬å¤„åªæœ‰ä¸€æ¡ï¼‰
                emb = output['embeddings'][0]
                if emb['embedding'] is None or len(emb['embedding']) == 0:
                    raise RuntimeError(f"DashScopeè¿”å›žçš„embeddingä¸ºç©ºï¼Œtext_index={emb.get('text_index', None)}")
                return emb['embedding']
            elif output and isinstance(output, dict) and 'embedding' in output:
                # å…¼å®¹å•æ¡è¾“å…¥æ ¼å¼
                if output['embedding'] is None or len(output['embedding']) == 0:
                    raise RuntimeError("DashScopeè¿”å›žçš„embeddingä¸ºç©º")
                return output['embedding']
            else:
                raise RuntimeError(f"DashScope embedding APIè¿”å›žæ ¼å¼å¼‚å¸¸: {rsp}")
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ embedding provider: {self.embedding_provider}")

    @staticmethod
    def set_up_llm():
        # é™æ€æ–¹æ³•ï¼Œåˆå§‹åŒ–OpenAI LLM
        load_dotenv()
        llm = OpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            timeout=None,
            max_retries=2
        )
        return llm

    def _load_dbs(self):
        # åŠ è½½æ‰€æœ‰å‘é‡åº“å’Œå¯¹åº”æ–‡æ¡£ï¼Œå»ºç«‹æ˜ å°„
        all_dbs = []
        # èŽ·å–æ‰€æœ‰JSONæ–‡æ¡£è·¯å¾„
        all_documents_paths = list(self.documents_dir.glob('*.json'))
        vector_db_files = {db_path.stem: db_path for db_path in self.vector_db_dir.glob('*.faiss')}
        
        for document_path in all_documents_paths:
            stem = document_path.stem
            if stem not in vector_db_files:
                _log.warning(f"No matching vector DB found for document {document_path.name}")
                continue
            try:
                with open(document_path, 'r', encoding='utf-8') as f:
                    document = json.load(f)
            except Exception as e:
                _log.error(f"Error loading JSON from {document_path.name}: {e}")
                continue
            
            # æ ¡éªŒæ–‡æ¡£ç»“æž„
            if not (isinstance(document, dict) and "metainfo" in document and "content" in document):
                _log.warning(f"Skipping {document_path.name}: does not match the expected schema.")
                continue
            
            try:
                vector_db = faiss.read_index(str(vector_db_files[stem]))
            except Exception as e:
                _log.error(f"Error reading vector DB for {document_path.name}: {e}")
                continue
                
            report = {
                "name": stem,
                "vector_db": vector_db,
                "document": document
            }
            all_dbs.append(report)
        return all_dbs

    @staticmethod
    def get_strings_cosine_similarity(str1, str2):
        # è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆé€šè¿‡åµŒå…¥ï¼‰
        llm = VectorRetriever.set_up_llm()
        embeddings = llm.embeddings.create(input=[str1, str2], model="text-embedding-3-large")
        embedding1 = embeddings.data[0].embedding
        embedding2 = embeddings.data[1].embedding
        similarity_score = np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))
        similarity_score = round(similarity_score, 4)
        return similarity_score

    def retrieve_by_company_name(self, company_name: str, query: str, llm_reranking_sample_size: int = None, top_n: int = 3, return_parent_pages: bool = False) -> List[Tuple[str, float]]:
        # æŒ‰å…¬å¸åæ£€ç´¢ç›¸å…³æ–‡æœ¬å—ï¼Œè¿”å›žå‘é‡è·ç¦»æœ€è¿‘çš„top_nä¸ªå—
        target_report = None
        for report in self.all_dbs:
            document = report.get("document", {})
            metainfo = document.get("metainfo")
            if not metainfo:
                _log.error(f"Report '{report.get('name')}' is missing 'metainfo'!")
                raise ValueError(f"Report '{report.get('name')}' is missing 'metainfo'!")
            if metainfo.get("company_name") == company_name:
                target_report = report
                break
        
        if target_report is None:
            _log.error(f"No report found with '{company_name}' company name.")
            raise ValueError(f"No report found with '{company_name}' company name.")
        
        document = target_report["document"]
        vector_db = target_report["vector_db"]
        chunks = document["content"]["chunks"]
        pages = document["content"]["pages"]
        
        actual_top_n = min(top_n, len(chunks))
        
        # èŽ·å– query çš„ embeddingï¼Œæ”¯æŒ openai/dashscope
        embedding = self._get_embedding(query)
        embedding_array = np.array(embedding, dtype=np.float32).reshape(1, -1)
        distances, indices = vector_db.search(x=embedding_array, k=actual_top_n)
    
        retrieval_results = []
        seen_pages = set()
        
        for distance, index in zip(distances[0], indices[0]):
            distance = round(float(distance), 4)
            chunk = chunks[index]
            parent_page = next(page for page in pages if page["page"] == chunk["page"])
            if return_parent_pages:
                if parent_page["page"] not in seen_pages:
                    seen_pages.add(parent_page["page"])
                    result = {
                        "distance": distance,
                        "page": parent_page["page"],
                        "text": parent_page["text"]
                    }
                    retrieval_results.append(result)
            else:
                result = {
                    "distance": distance,
                    "page": chunk["page"],
                    "text": chunk["text"]
                }
                retrieval_results.append(result)
            
        return retrieval_results

    def retrieve_all(self, company_name: str) -> List[Dict]:
        # æ£€ç´¢å…¬å¸æ‰€æœ‰æ–‡æœ¬å—ï¼Œè¿”å›žå…¨éƒ¨å†…å®¹
        target_report = None
        for report in self.all_dbs:
            document = report.get("document", {})
            metainfo = document.get("metainfo")
            if not metainfo:
                continue
            if metainfo.get("company_name") == company_name:
                target_report = report
                break
        
        if target_report is None:
            _log.error(f"No report found with '{company_name}' company name.")
            raise ValueError(f"No report found with '{company_name}' company name.")
        
        document = target_report["document"]
        pages = document["content"]["pages"]
        
        all_pages = []
        for page in sorted(pages, key=lambda p: p["page"]):
            result = {
                "distance": 0.5,
                "page": page["page"],
                "text": page["text"]
            }
            all_pages.append(result)
            
        return all_pages


class HybridRetriever:
    def __init__(self, vector_db_dir: Path, documents_dir: Path):
        self.vector_retriever = VectorRetriever(vector_db_dir, documents_dir)
        self.reranker = LLMReranker()
        
    def retrieve_by_company_name(
        self, 
        company_name: str, 
        query: str, 
        llm_reranking_sample_size: int = 28,
        documents_batch_size: int = 2,
        top_n: int = 6,
        llm_weight: float = 0.7,
        return_parent_pages: bool = False
    ) -> List[Dict]:
        """
        Retrieve and rerank documents using hybrid approach.
        
        Args:
            company_name: Name of the company to search documents for
            query: Search query
            llm_reranking_sample_size: Number of initial results to retrieve from vector DB
            documents_batch_size: Number of documents to analyze in one LLM prompt
            top_n: Number of final results to return after reranking
            llm_weight: Weight given to LLM scores (0-1)
            return_parent_pages: Whether to return full pages instead of chunks
            
        Returns:
            List of reranked document dictionaries with scores
        """
        # Get initial results from vector retriever
        vector_results = self.vector_retriever.retrieve_by_company_name(
            company_name=company_name,
            query=query,
            top_n=llm_reranking_sample_size,
            return_parent_pages=return_parent_pages
        )
        
        # Rerank results using LLM
        reranked_results = self.reranker.rerank_documents(
            query=query,
            documents=vector_results,
            documents_batch_size=documents_batch_size,
            llm_weight=llm_weight
        )
        
        return reranked_results[:top_n]
